{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Imports-Cell"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import sympy as sp\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "seed = 42\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Part1-Header"
   },
   "source": [
    "# Part 1 - MLP for Regression (Spotify Song Popularity)\n",
    "\n",
    "In this part, we will build a **4-layer** MLP to predict song popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spotify-data-load"
   },
   "source": [
    "## 1.1 Data Loading and Preprocessing\n",
    "\n",
    "This section prepares the dataset for a regression task on Spotify song popularity.\n",
    "\n",
    "It includes feature selection, data cleaning, normalization, and train‚Äìtest splitting,formatted to match the expected input structure of a fully connected neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "spotify-data-code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_reg shape: (13, 24000)\n",
      "y_train_reg shape: (1, 24000)\n",
      "(Input features, 'I' = 13)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    spotify_df = pd.read_csv('tracks.csv')\n",
    "    \n",
    "    # The tracks.csv has 'release_date', so we create 'year' from it\n",
    "    if 'year' not in spotify_df.columns and 'release_date' in spotify_df.columns:\n",
    "        spotify_df['year'] = pd.to_datetime(spotify_df['release_date'], errors='coerce').dt.year\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset file 'tracks.csv' not found.\")\n",
    "    print(\"Falling back to a synthetic dataset for pipeline validation.\")\n",
    "    spotify_df = pd.DataFrame(np.random.rand(100, 14), \n",
    "                              columns=['acousticness', 'danceability', 'energy', 'instrumentalness', \n",
    "                                       'liveness', 'loudness', 'speechiness', 'tempo', 'valence', 'popularity', \n",
    "                                       'key', 'mode', 'explicit', 'year'])\n",
    "\n",
    "# Select features (X) and target (Y)\n",
    "features = ['acousticness', 'danceability', 'energy', 'instrumentalness', \n",
    "            'liveness', 'loudness', 'speechiness', 'tempo', 'valence', \n",
    "            'key', 'mode', 'explicit', 'year']\n",
    "target = 'popularity'\n",
    "\n",
    "# Drop NAs\n",
    "spotify_df = spotify_df.dropna(subset=features + [target])\n",
    "\n",
    "# Ensure numeric types\n",
    "for col in features + [target]:\n",
    "    spotify_df[col] = pd.to_numeric(spotify_df[col], errors='coerce')\n",
    "    \n",
    "spotify_df = spotify_df.dropna()\n",
    "\n",
    "# For performance, we'll sample 30,000 tracks\n",
    "if len(spotify_df) > 30000:\n",
    "    spotify_df = spotify_df.sample(n=30000, random_state=seed)\n",
    "\n",
    "X_spotify = spotify_df[features]\n",
    "Y_spotify = spotify_df[[target]]\n",
    "\n",
    "# Scale the data\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_Y = MinMaxScaler()\n",
    "X_spotify_scaled = scaler_X.fit_transform(X_spotify)\n",
    "Y_spotify_scaled = scaler_Y.fit_transform(Y_spotify)\n",
    "\n",
    "# Split data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_spotify_scaled, Y_spotify_scaled, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Transpose for our NN architecture: (features, num_examples)\n",
    "x_train_reg = X_train_reg.T\n",
    "y_train_reg = y_train_reg.T\n",
    "x_test_reg = X_test_reg.T\n",
    "y_test_reg = y_test_reg.T\n",
    "\n",
    "print(f\"X_train_reg shape: {x_train_reg.shape}\")\n",
    "print(f\"y_train_reg shape: {y_train_reg.shape}\")\n",
    "print(f\"(Input features, 'I' = {x_train_reg.shape[0]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 MLP Architecture\n",
    "\n",
    "**Architecture:** `input - FC - ReLU - FC - ReLU - FC - ReLU - FC - Output`\n",
    "\n",
    "The output activation is also ReLU (for Regression) to ensure our prediction `popularity` is non-negative.\n",
    "\n",
    "**Dimensions (Example):**\n",
    "* `I`: Input features (13 for Spotify)\n",
    "* `H1`: 64 neurons\n",
    "* `H2`: 32 neurons\n",
    "* `H3`: 16 neurons\n",
    "* `O`: Output Layer (1 for this regression task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Helper: Parameter Initialization (He Initialization)\n",
    "\n",
    "To ensure the network learns effectively using ReLU activations, we must initialize weights carefully. If weights are too small, gradients vanish. If too large, they explode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Initialization(I, H1, H2, H3, O):\n",
    "    '''\n",
    "    Initializes weights using He Initialization (optimized for ReLU).\n",
    "    \n",
    "    Inputs:\n",
    "        I, H1, H2, H3, O: Number of neurons in each layer.\n",
    "        \n",
    "    Returns:\n",
    "        List of parameters: [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "    '''\n",
    "    np.random.seed(42) # Ensure reproducibility\n",
    "\n",
    "    # Calculate scaling factors for He Initialization\n",
    "    scale1 = np.sqrt(2.0 / I)\n",
    "    scale2 = np.sqrt(2.0 / H1)\n",
    "    scale3 = np.sqrt(2.0 / H2)\n",
    "    scale4 = np.sqrt(2.0 / H3)\n",
    "    \n",
    "    # Layer 1: Input -> Hidden 1\n",
    "    W1 = np.random.randn(H1, I) * scale1\n",
    "    b1 = np.zeros((H1, 1)) \n",
    "    \n",
    "    # Layer 2: Hidden 1 -> Hidden 2\n",
    "    W2 = np.random.randn(H2, H1) * scale2\n",
    "    b2 = np.zeros((H2, 1))\n",
    "    \n",
    "    # Layer 3: Hidden 2 -> Hidden 3\n",
    "    W3 = np.random.randn(H3, H2) * scale3\n",
    "    b3 = np.zeros((H3, 1))\n",
    "    \n",
    "    # Layer 4: Hidden 3 -> Output\n",
    "    W4 = np.random.randn(O, H3) * scale4\n",
    "    b4 = np.zeros((O, 1))\n",
    "    \n",
    "    print(\"Parameters initialized with He Initialization.\")\n",
    "    return [W1, b1, W2, b2, W3, b3, W4, b4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters initialized with He Initialization.\n",
      "Sample Test passed üëç\n"
     ]
    }
   ],
   "source": [
    "#Sample test case\n",
    "np.random.seed(seed)\n",
    "params_list = Initialization(13, 64, 32, 16, 1)\n",
    "assert len(params_list) == 8\n",
    "assert params_list[0].shape == (64, 13) \n",
    "assert params_list[1].shape == (64, 1)  \n",
    "assert params_list[6].shape == (1, 16)  \n",
    "assert np.mean(params_list[0]) < 0.1 and np.mean(params_list[0]) > -0.1\n",
    "print('Sample Test passed', '\\U0001F44D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Activation Function (ReLU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x: activation input of shape (n_units, m)\n",
    "def relu(x):\n",
    "    # Element-wise ReLU activation\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def drelu(x):\n",
    "    # Boolean mask for element-wise derivative of ReLU\n",
    "    return (x > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Test passed üëç\n"
     ]
    }
   ],
   "source": [
    "# sample test case\n",
    "x_sample = np.array([[3, -4], [0, -0.5]])   #two samples with the values of its two fetchers.\n",
    "assert np.allclose(relu(x_sample), np.array([[3, 0], [0, 0]]))\n",
    "assert np.allclose(drelu(x_sample), np.array([[1, 0], [0, 0]]))\n",
    "print('Sample Test passed', '\\U0001F44D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Forward Propagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def feed_forward(X, params):\n",
    "    '''\n",
    "    Inputs:\n",
    "    - X : Input data of shape (I, num_examples)\n",
    "    - params: List of parameters [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "\n",
    "    Outputs:\n",
    "    - y_out : predicted output (A4), shape (O, num_examples)\n",
    "    - cache : List containing [Z1, A1, Z2, A2, Z3, A3, Z4, A4]\n",
    "    '''\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = params\n",
    "    cache = []\n",
    "    A0 = X\n",
    "    Z1 = W1 @ A0 + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = W2 @ A1 + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = W3 @ A2 + b3\n",
    "    A3 = relu(Z3)\n",
    "    Z4 = W4 @ A3 + b4\n",
    "    A4 = relu(Z4)\n",
    "    for x in [Z1,A1,Z2,A2,Z3,A3,Z4,A4]:\n",
    "        cache.append(x)\n",
    "\n",
    "    y_out = A4 \n",
    "    return y_out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters initialized with He Initialization.\n",
      "Sample Test passed üëç\n"
     ]
    }
   ],
   "source": [
    "#Sample test case\n",
    "np.random.seed(seed)\n",
    "I, H1, H2, H3, O, m = 13, 64, 32, 16, 1, 100\n",
    "params_list = Initialization(I, H1, H2, H3, O)\n",
    "X_sample = np.random.rand(I, m)\n",
    "y_forward, cache = feed_forward(X_sample, params_list)\n",
    "\n",
    "assert y_forward.shape == (O, m)\n",
    "assert len(cache) == 8 \n",
    "assert cache[0].shape == (H1, m) \n",
    "assert cache[7].shape == (O, m) \n",
    "print('Sample Test passed', '\\U0001F44D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6: Loss Function (MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_compute(y_pred, yd):\n",
    "    '''\n",
    "    Inputs:\n",
    "    - y_pred: (O, m) numpy array of predicted values\n",
    "    - yd: (O, m) numpy array of true values\n",
    "\n",
    "    Outputs:\n",
    "    - loss: The MSE loss (scalar)\n",
    "    '''\n",
    "    m = yd.shape[1]\n",
    "    loss = 1/(2*m) * np.sum((y_pred - yd)**2)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333333\n",
      "Sample Test passed üëç\n"
     ]
    }
   ],
   "source": [
    "sample_loss = loss_compute(np.array([[1, 1, 4]]), np.array([[1, 0, 2]]))\n",
    "print(sample_loss)\n",
    "assert np.allclose(sample_loss, 0.8333333333333334)\n",
    "\n",
    "print('Sample Test passed', '\\U0001F44D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization_L2(lmbda, W1, W2, W3, W4, m):\n",
    "    '''\n",
    "    Inputs:\n",
    "    - lmbda: Regularization parameter (lambda)\n",
    "    - W1, W2, W3, W4: Weight matrices\n",
    "    - m: number of examples\n",
    "\n",
    "    Outputs:\n",
    "    - reg_loss: The L2 regularization cost (scalar)\n",
    "    '''\n",
    "\n",
    "    reg_loss = (lmbda/(2*m))*np.sum(np.linalg.norm(W1, axis=1)**2 + np.linalg.norm(W2, axis=1)**2 +np.linalg.norm(W3, axis=1)**2 + np.linalg.norm(W4, axis=1)**2)\n",
    "    return reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Test passed üëç\n"
     ]
    }
   ],
   "source": [
    "# TEST CASE 1\n",
    "w1 = np.array([[1, 1], [1, 1]])\n",
    "w2 = np.array([[1, 1], [1, 1]])\n",
    "w3, w4 = w1.copy(), w2.copy()\n",
    "# print(regularization_L2(0.1, w1, w2, w3, w4, 2))\n",
    "assert np.allclose(regularization_L2(0.1, w1, w2, w3, w4, 2), 0.4)\n",
    "print('Sample Test passed', '\\U0001F44D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Backpropagation_reg(X, yd, cache, parameters, lmbda):\n",
    "    '''\n",
    "    Performs the backward pass for the 4-layer network (Regression version).\n",
    "\n",
    "    Inputs:\n",
    "      - X: Input features (I, m)\n",
    "      - yd: True labels (O, m)\n",
    "      - cache: List from feed_forward [Z1, A1, Z2, A2, Z3, A3, Z4, A4]\n",
    "      - parameters: List of parameters [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "      - lmbda: regularization parameter\n",
    "\n",
    "    Outputs:\n",
    "      - grads: Dictionary of gradients { \"dW1\": dW1, \"db1\": db1, ... }\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = parameters\n",
    "    Z1, A1, Z2, A2, Z3, A3, Z4, A4 = cache\n",
    "    grads = {}\n",
    "\n",
    "    # Layer 4\n",
    "    dA4 = (1/m) * (A4 - yd)               # gradient of loss w.r.t A4\n",
    "    dZ4 = dA4 * drelu(Z4)                 # element-wise\n",
    "    dW4 = dZ4 @ A3.T + (lmbda/m) * W4\n",
    "    db4 = np.sum(dZ4, axis=1, keepdims=True)\n",
    "    # propagate to previous layer\n",
    "    dA3 = W4.T @ dZ4                      # gradient w.r.t A3\n",
    "\n",
    "    # Layer 3\n",
    "    dZ3 = dA3 * drelu(Z3)\n",
    "    dW3 = dZ3 @ A2.T + (lmbda/m) * W3\n",
    "    db3 = np.sum(dZ3, axis=1, keepdims=True)\n",
    "    dA2 = W3.T @ dZ3\n",
    "\n",
    "    # Layer 2\n",
    "    dZ2 = dA2 * drelu(Z2)\n",
    "    dW2 = dZ2 @ A1.T + (lmbda/m) * W2\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dA1 = W2.T @ dZ2\n",
    "\n",
    "    # Layer 1\n",
    "    dZ1 = dA1 * drelu(Z1)\n",
    "    dW1 = dZ1 @ X.T + (lmbda/m) * W1       # A0 = X\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    grads['dW1'] = dW1\n",
    "    grads['db1'] = db1\n",
    "    grads['dW2'] = dW2\n",
    "    grads['db2'] = db2\n",
    "    grads['dW3'] = dW3\n",
    "    grads['db3'] = db3\n",
    "    grads['dW4'] = dW4\n",
    "    grads['db4'] = db4\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters initialized with He Initialization.\n",
      "Sample Test passed üëç\n"
     ]
    }
   ],
   "source": [
    "#Sample test case\n",
    "np.random.seed(seed)\n",
    "I, H1, H2, H3, O, m = 13, 64, 32, 16, 1, 100\n",
    "params_list = Initialization(I, H1, H2, H3, O)\n",
    "X_sample = np.random.rand(I, m)\n",
    "Y_sample = np.random.rand(O, m)\n",
    "y_forward, cache = feed_forward(X_sample, params_list)\n",
    "grads = Backpropagation_reg(X_sample, Y_sample, cache, params_list, lmbda=0.1)\n",
    "\n",
    "assert grads['dW1'].shape == (H1, I)\n",
    "assert grads['db1'].shape == (H1, 1)\n",
    "assert grads['dW4'].shape == (O, H3)\n",
    "assert grads['db4'].shape == (O, 1)\n",
    "print('Sample Test passed', '\\U0001F44D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
